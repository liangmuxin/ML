{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: 'pROC'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "Loading required package: ggplot2\n",
      "Loading required package: lattice\n",
      "Loading required package: gplots\n",
      "\n",
      "Attaching package: 'gplots'\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    lowess\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(LiblineaR)\n",
    "library(pROC)\n",
    "library(useful)\n",
    "library(caret)\n",
    "library(ROCR)\n",
    "library(clue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 5 178 5 598 5 150 5 748 5"
     ]
    }
   ],
   "source": [
    "#readdata, get train,test data\n",
    "alldata= read.csv(\"transfusion.data\")\n",
    "colnames(alldata) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "ones=which(alldata$Class==1)\n",
    "zeros=which(alldata$Class==0)\n",
    "\n",
    "classone<-alldata[ones,]\n",
    "classzero<-alldata[zeros,]\n",
    "rownames(classone) <- seq(length=nrow(classone))\n",
    "rownames(classzero) <- seq(length=nrow(classzero))\n",
    "\n",
    "zeroind=ceiling(1/5*nrow(classzero))\n",
    "oneind=ceiling(1/5*nrow(classone))\n",
    "\n",
    "test = rbind(classzero[1:zeroind,],classone[1:oneind,])\n",
    "train=rbind(classzero[(zeroind+1):nrow(classzero),],classone[(oneind+1):nrow(classone),])\n",
    "\n",
    "cat(dim(classzero),dim(classone),dim(train),dim(test), dim(alldata))\n",
    "\n",
    "rownames(train) <- seq(length=nrow(train))\n",
    "rownames(test) <- seq(length=nrow(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervised learning\n",
    "strain<-scale(train[1:4],center = TRUE, scale = TRUE)\n",
    "trainnorm=cbind(strain,train$Class)\n",
    "stest<-scale(test[1:4],center = TRUE, scale = TRUE)\n",
    "testnorm=cbind(stest,test$Class)\n",
    "colnames(trainnorm) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "colnames(testnorm)<-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "trainnorm=as.data.frame(trainnorm)\n",
    "testnorm=as.data.frame(testnorm)\n",
    "# trainnorm$Class=as.factor(trainnorm$Class)\n",
    "# testnorm$Class=as.factor(testnorm$Class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryCosts= c(1000,100,10,1,0.1,0.01,0.001)\n",
    "bestCo= NA\n",
    "bestAcc= 0\n",
    "\n",
    "for(co in tryCosts){\n",
    "acc=LiblineaR(data=trainnorm[,1:4],target=factor(trainnorm$Class),type=5,cost=co,bias=1,cross=5,verbose=FALSE)\n",
    "# cat(\"Results for C=\",co,\" : \",1-acc,\" error.\\n\",sep=\"\")\n",
    "if(acc>bestAcc){\n",
    "bestCost=co\n",
    "bestAcc=acc\n",
    "}\n",
    "    }\n",
    "model =LiblineaR(data=trainnorm[,1:4],target=factor(trainnorm$Class),type=5,cost=bestCost,bias=1,verbose=FALSE)\n",
    "\n",
    "\n",
    "# Display confusion matrix\n",
    "# res=table(p$predictions,testnorm$Class)\n",
    "# c=confusionMatrix(p$predictions,testnorm$Class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roccurve <- roc(testnorm$Class ~ptrain$predictions)\n",
    "\n",
    "ptrain=predict(model, newx=trainnorm[,1:4],decisionValues=TRUE)\n",
    "ptest=predict(model,newx=testnorm[,1:4],decisionValues=TRUE)\n",
    "# pd <- prediction(ptrain,as.numeric(trainnorm$Class)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 449 126\n",
       "         1   7  16\n",
       "                                          \n",
       "               Accuracy : 0.7776          \n",
       "                 95% CI : (0.7421, 0.8103)\n",
       "    No Information Rate : 0.7625          \n",
       "    P-Value [Acc > NIR] : 0.2078          \n",
       "                                          \n",
       "                  Kappa : 0.1368          \n",
       " Mcnemar's Test P-Value : <2e-16          \n",
       "                                          \n",
       "            Sensitivity : 0.9846          \n",
       "            Specificity : 0.1127          \n",
       "         Pos Pred Value : 0.7809          \n",
       "         Neg Pred Value : 0.6957          \n",
       "             Prevalence : 0.7625          \n",
       "         Detection Rate : 0.7508          \n",
       "   Detection Prevalence : 0.9615          \n",
       "      Balanced Accuracy : 0.5487          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAY1BMVEUAAAAzMzNCQkJNTU1a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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusionMatrix(ptrain$predictions,trainnorm$Class)\n",
    "roccurve<-roc(trainnorm$Class~ptrain$decisionValues[,1])\n",
    "# plot(roccurve,xlab=\"False Positive Rate\",ylab=\"True Positive Rate\")\n",
    "plot(roccurve,xlab=\"False Positive Rate\",ylab=\"True Positive Rate\",auc.polygon = TRUE,print.auc=TRUE)\n",
    "\n",
    "# legend(0.9,toString(auc(roccurve)),lwd=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 114  34\n",
       "         1   0   2\n",
       "                                          \n",
       "               Accuracy : 0.7733          \n",
       "                 95% CI : (0.6979, 0.8376)\n",
       "    No Information Rate : 0.76            \n",
       "    P-Value [Acc > NIR] : 0.3931          \n",
       "                                          \n",
       "                  Kappa : 0.0821          \n",
       " Mcnemar's Test P-Value : 1.519e-08       \n",
       "                                          \n",
       "            Sensitivity : 1.00000         \n",
       "            Specificity : 0.05556         \n",
       "         Pos Pred Value : 0.77027         \n",
       "         Neg Pred Value : 1.00000         \n",
       "             Prevalence : 0.76000         \n",
       "         Detection Rate : 0.76000         \n",
       "   Detection Prevalence : 0.98667         \n",
       "      Balanced Accuracy : 0.52778         \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAXVBMVEUAAABCQkJNTU1aWlpo\naGhra2t5eXl8fHyFhYWMjIyQkJCampqjo6Onp6epqamrq6uysrKzs7O7u7u9vb3CwsLHx8fJ\nycnPz8/Q0NDZ2dnc3Nzh4eHp6enw8PD///8tiZSJAAAACXBIWXMAABJ0AAASdAHeZh94AAAg\nAElEQVR4nO3diZbaxhZA0bKCjTEPm4S0DWH4/898SGKQENAabt26VXX2yrLpAVWH1rFm4U4A\nJnOhfwAgBYQECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIE\nEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIE\nEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIE\nEBIggJAAAQohOSAuP0fM5fLhBBgCEPRzxCxLSEDbzzGzLCEBLT9HzbKEBDSV20eEBExT7Wcg\nJGCSen8dIQFTXPZ7ExIwwfX4ESEB492OwxISMNr9fAZCAsZqnBekGtJuvajOS1qsdr6GANQ0\nz69TDOk4a5zjN/cyBKCndZ6qYkgrV3zsq0eHbeFWPoYA1LTP91YMqXD72+O9K3wMAWh5uG5C\nMSTnXn0gNgSg5PH6I5ZIwHCd6/h0t5G2h+oR20iIW/d6WM3d3/PGXrvZ0csQgIIn15XrHkda\nVceRisWa40iI17P7M3BmAzDM0/ucEBIwyPP7BRESMMSL+26FConjSHlQvLOchpcdGQrp4QdG\nCgLN7/68vA8kq3bwyLn/kvL6fqqEBI8SC+nN/VQJCR6lFdLP/wgJQSQV0s//CAlhpBTSuSNC\nQhgJhVR2ZCOkhx2JPoaAMemEVHVkI6QNIWUnmZDqjmyEdNoX7295IjAEbEklpEtHRkI67d9f\nzicxBExJJKRrR1ZCOq/d7T//pmlDwJI0Qrp1ZCYkQ0NAQxIh3TsiJISRQkiNjggJYSQQUrMj\nQkIY8YfU6oiQEEb0IbU7IiSouxx3DzP/S3noiJCgzaUQ0mNHhARtkSdU6XRESNCWQEjdjggJ\n2uIP6UlHhARt0Yf0rCNCgrbYQ3raESFBW+QhPe+IkKAt7pBedERI0BZ1SK86IiRoizmklx0R\nErRFHNLrjggJ2uIN6U1HhARt0Yb0riNCgrZYQ3rbESFBW6Qhve+IkKAtzpA+6YiQoC3KkD7r\niJCgLcaQPu2IkKAtwpA+74iQoC2+kHp0REjQFl1IfToiJGiLLaReHREStEUWUr+OCAna4gqp\nZ0eEBG1RhdS3I0KCtphC6t0RIUFbRCH174iQoC2ekAZ0REjQFk1IQzoiJGiLJaRBHREStEUS\n0rCOCAna4ghpYEeEBG1RhDS0I0KCthhCGtwRIUFbBCEN74iQoM1+SCM6IiRoMx/SmI4ICdqs\nhzSqI0KCNuMhjeuIkKDNdkgjOyIkaDMd0tiOCAnaLIc0uiNCgjbDIY3viJCgzW5IEzoiJGgz\nG9KUjggJ2qyGNKkjQoI2oyFN64iQoM1mSBM7IiRoMxnS1I4ICdoshjS5I0KCNoMhTe+IkKDN\nXkgCHREStJkLSaIjQoI2ayGJdERI0GYsJJmOCAnabIUk1BEhQZupkKQ6IiRosxSSWEeEBG2G\nQpLriJCgzU5Igh0RErSZCUmyI0KCNishiXZESNBmJCTZjggJ2myEJNwRIUGbiZCkOyIkaLMQ\nknhHhARtBkKS74iQoC18SB46IiRoCx6Sj44ICdpCh+SlI0KCJlfxMif35acjQoIiFz4kTx0R\nEhSFXqvz1xEhQVHwkLx1REhQFDokfx0REhSluZ+hQkjQk+Z+hgohQU/QkLx2REhQFDIkvx0R\nEhQFDMlzR4QEReFC8t0RIUFRsJC8d0RIUBQqJP8dERIUBQpJoSNCgqIwIWl0REhQFCQklY4I\nCYpChKTTESFBUYCQlDoiJHjkOpRm6xutjggJ/nQ70g5JrSNCgj+hLz9S7IiQ4E/okBQ7IiT4\nk/B1fB2EBG8Svo6vg5DgTcLX8XUQErxJ+Dq+DkKCNwlfx9dhI6TD0hXr02kzc8XK0xDQl/B1\nfB0mQjoW5bG6zbo6ZDf3MgQCSPg6vg4TIa3ceTm0KtzyeDpWj+WHQAAJX8fXYSKkwtUv+7H6\nq/AxBAJI+Dq+DhMhOXf/8/qX8BAIIOHr+DpMhFQ0QjqyREpGwtfxdZgI6bqNtDpeHssPgQAS\nvo6vw0RI7LVLU8LX8XWYCInjSGlK+Dq+DhshmRoCk+RxHV8HIUFUyAtiA3ZESJCV0/l1TfZC\n4jhS1HI6v64phpBaKwsSQ8CfnM6va7IXUvAhMEVO59c1ERJE5XR+XRMhQVRO59c1GQlpt15U\nW0CL1c7XEFCR0/l1TSZCOs4aexM4RShqOZ1f12QipJUrPvbVo8O24KTVqOV0fl2TiZAKt789\n3nMZRdRyOr+uyURIraNDHJCNWk7n1zWZCIklUjpyOr+uyURI522k7aF6xDZS7JRDstKRjZBO\n88Zeu9nRyxDQoRuSmY6MhHTararjSMVizXGkuKmGZKcjKyFZGgJTaIZkqCNCgizFkCx1REiQ\npReSqY4ICbLUQrLVESFBllZIxjoiJMhSCslaR4QEWTohmeuIkCBLJSR7HRESZGmEZLAjQoIs\nhZAsdkRIkOU/JJMdERJkeQ/JZkeEBFm+QzLaESFBlueQrHZESJDlNySzHRESZHkNyW5HhARZ\nPkMy3BEhQZbHkCx3REiQ5S8k0x0REmR5C8l2R4QEWb5CMt4RIUGWp5Csd0RIkOUnJPMdERJk\neQnJfkeEBFk+QoqgI0KCLA8hxdARIUGWfEhRdERIkCUeUhwdERJkSYcUSUeEBFnCIcXSESFB\nlmxI0XRESJAlGlI8HRESZEmGFFFHhARZgiHF1BEhQZZcSFF1REiQJRZSXB0REuTUb0svM2NG\n1hEhQYwTDCm2jggJYnLdz1AhJEjJdT9DhZAgJdf9DBVCgpRc9zNUCAlSct3PUCEkSJEJKc6O\nCAliREKKtCNCghiJkGLtiJAgRiCkaDsiJIiZHlK8HRESxEwOKeKOCAlipoYUc0eEBDETQ4q6\nI0KCmGkhxd0RIUHMpJAi74iQIGZKSLF3REgYzXWMng2j74iQMFa3o9Ehxd8RIWGsrK/j6yAk\njJT1dXwdhISRsr6Or4OQMFLW1/F1EBJGyvo6vg5CwkhZX8fXQUgYKevr+DoICSNlfR1fByFh\npKyv4+sgJIyU9XV8HYSEkbK+jq+DkDBS1tfxdRASRsr6Or4OQsJIWV/H10FIGCnr6/g6CAkj\nZX0dXwchCXhyiVsWxs50CXZESAJCz8/BjJznUuyIkAQIvndqDpLsiJAEENIQaXZESAIIaYBE\nOyIkAYTUX6odEZIAQuot2Y4ISQAh9ZVuR4QkgJB6SrgjQhJASP2k3BEhCSCkXpLuiJAEEFIf\naXdESAIIqYfEOyIkAYT0udQ7IiQBhPSp5DsiJAGE9Jn0OyIkAYT0iQw6IiQBhPReDh0R0sDh\nRa9wy0MWHRHSsNEJabA8OiKkYaOTzFCZdERIw0YnpIFy6YiQBg1ORwNl0xEhDRqckIbJpyNC\nGjQ4IQ2SUUeENGRsOhokp460Qtou3PmLi8PwafYewj9CGiSrjpRCmpcHW85fLyaXREixyKsj\nnZA2bn4sQ9q45fCJ9htCAR0NkVlHOiEV7ngqQ6r/mISQ4pBbRzohVat170I6rorzn+uZc/OP\ncUMoIKT+sutIJ6TZZYm0d7OnXz8U568ei/q8tfmoIfyjo/7y60h1G2lbuM3Try/d4nj+Y3k4\nN7V0qzFD+EdIvWXYkdJeu4V7u7Rx5yVW/cd5Lc8Vo4bwjpD6yrEjzeNIbvFq+6fadCpc4wPJ\nn0oGHfWVZUc2zmxYuv3ptC7/KJdIbzeSvIbk5f3pcpNnRzZC2rtitT8tinNJ25nb+hiiDzoS\nkGlHeru/K8WL7Z9tcZ9h16OGkEAs0+XakW5Ih9fbPx/LWVnRYv3JSUQeQ6Kj6bLtyH9I29b6\n0fPjSBOHEEJIk+XbkcISadbsaDd8opN/qr5TpqOpMu5IeRtpOm8hsTthspw7srHXrj2RMMeR\n6GiqrDtSDmm36DGRzlRaW1nDf6pe6GiqvDtSCmkl1oGnkFixmyrzjnRCunf09mDrlCGmTpaO\npsm9I60L+z5Oc3c4zJ3RvXZ0NFH2HenttVufl0b7l+fR7db1CeKL1SepEZJFdKQX0ra8FunF\nNtKxeawpwIV9dDQNHSmFtDiv2h3c7LR7EdLKFR/Vqd+nw7bQv7CPPQ3T0NF/SiFty4CqW3I9\nv4tQUV9BUdnrX9hHR5PQUUln9/e6/MrSvVrYtBZU6gdk6WgSOqqYOLMh7BKJkKago5qJkM7b\nSNv68okA20h0NAUdXeiGtH9xitC8eYb4cdIQQ7GnYQo6uvIf0u5cybxaddsvXm7/7FbVcaRi\nsdY+jkRHE9DRjfeQdvVyZn86lKm8XW0bO8S0CRLSaHR05z2keRnPys3LK2UXb9faRg8xbYKE\nNBYdNXgPqV6bO6+1ucX+yZclhpg2QUIaiY6a1EKafpX5yyGmTZCQxqGjFrWQhk+t9xDTJkhI\no9BRGyER0hh09ICQCGkEOnqkEJLoLRcIyQI66iAkQhqMjrpMnGsXcghCGoyOniAkQhqIjp4h\nJEIaho6eIiRCGoSOniMkQhqCjl4gJEIagI5eISRC6o+OXiIkQuqNjl4jJELqi47eUAppW11l\nvvjkDWInDTF2goTUEx29oxPSvD47yBWTSyKkUOjoLZWQNm5+LEPavLjTqsAQ4ydISL3Q0Xsq\nIRXuWF9IwUmrsaKjT6iEVK3WEVLE6OgzKiHNLkukvZsNn2i/IcZPkJA+R0ef0txG2hbleyRN\nQ0gB0NHndPbaLXq9idikIUZPkJA+Q0c9KB5HcouP4ZPsP8TYCRLSJ+ioD85sIKT36KgXlZCm\n36n40yHGT5CQ3qKjfnR2f8+3wyc2bIjxEySkd+ioJ6Xd386tuGVxhOioL51tpMP63NJsLbCK\nR0ia6Kg3tZ0Nh1XhBFbxCEkRHfWnudduY+wGkZeDW6F/BWbR0QB6S6Rq7W7ykSTBkBwhvUVH\nQyhuIxWr6df1yYYU+sU3jY4GUdtrtzS3146Q3qGjYZSOIwmcHPR+iDGTIqTX6GigjM9sIKTX\n6Ggo7yHVF/VZfFsXQnqJjgYjJHTQ0XAZn/1NSC/Q0QiEhAd0NIbazU8qRTF8ov2GGDMpQnqG\njkZRDenANpJ5dDSO95C2rfditnQXIUJ6go5G8r9EmjU7mnx6AyF5RUdj6W4jTUdIPtHRaOy1\nww0djccBWVzR0QSEhAs6moJVO9ToaBJCQoWOptEJaTM7nQ4zgb3fhORJ3dEX96X+8PbaXB78\n+/2L+/73k+f9OD/lx5/bh7dV+MvH/9xf43/SfrlVQtqW20ZF+fpyHMmmuqO/z7+hupaHkH7U\ndfz1+/F5X+vP3z6+dnTp8c+X22vceJgklZDm7qN6b6SP6W9HQUg+XNbrvrsf7nv1qB3S0n05\n9/Xn/NdDSf+4L//+9+8X90/7039fP/Ht/hp/S/zlVjsgu3criSOzhOTBdfvovCC5LDdaIf2+\nBvT9ktnNj2oB9sstW5/98+Vb/eDXfR2v8TBNaiEt3JaQbLp29Mv9OKfxq3zYCunHNZQ/3/7X\n/Fq5mCkL+9d9a03vm6s3mn67r9fvbTxMlNKq3X7rihOrdibd9td9Pa+R/eO+lg9bIX11/za/\nv/G6Off4mf/Krn5cp/f7+pXGw0Rp7Wxwbl3OupZuWZz4b7avW0d/ql0EX6rFSSuk1y/U05Cu\nC6TleeF2+UrjYaqUdn8X5RbSafqNVglJ2v340a9qSVKv200I6d/LhlS1wld/pfEwWRyQzVvj\nOOxf1b62f6ud2RNC+nHZg/7Xlz/XrzQeJouQstbo6PftYOrvh5C+3baR/v7TfvrjPr7G575X\nPVVfaTxMl1JIH3N7b8ac+G+2j+Z5QctbSMty6VTv8P5dLp+W1712/zQOvVbqvXa/m3vtrrvw\n7ucpNx+mSyek+eWFnLzTjpAktc6va7Xz3f2v+uB/5QbP7TjS18tnb5bVsubv6266+hn19xDS\nhFn21VM2rih3120Ltxk+0X5DjJlUyr/XPlod3Q8GlXu7/3ZVD79ctd30vTqz4fe365k/N0/O\nbPjW2lfe3VWeKpWQZm5f/b3n5ieGtM/3vu4juCxgLmfXXZY1X5vn2jVft7+qzzcPPf3lmttR\nhOTnng2c2WDHw3UTX760H/797VzIt2tcv84fff1VP26+bn+qs7+bn26/qoTkaYnEDSKt4Poj\nWWwj5YmOhLHXLkt0JE3rONKC40iG0JE4zmzIEB3JI6T80JEH/kM6rApXrKTeRpaQJqMjH7yH\ndKhueuKKw/DJ9R1i7KTyDImOvPAe0tLNj6fj3C2HT67vEGMnlWVIdOSH95AKV67VHaYfin09\nxNhJ5RgSHXmicu/v+1+TEdIkdOQLIeWEjrwhpIzQkT+ElA868kghpJbhE538U72cVGYh0ZFP\nhJQLOvKKU4QyQUd+EVIe6MgzQsoCHflGSDmgI+8IKQN05B8hpY+OFBBS8uhIAyGljo5UEFLi\n6EiHUkjbRfU+stMvkyWkYehIieJ97c5fn37BOSENQkdalO60Oj+WIW2mX3BOSEPQkRqVkMrL\nzavzVTlpVRUd6VEJqVqtIyRtdKRIJaTZZYnE+yNpoiNNmttIvBuFJjpSpbPXbsG7UWijI12K\nx5F4NwpFdKTM2JkNn+6NIKRe6EgbIaWIjtSZCGnADVKm7z9vCP3i+0JH+pSOI72PZFeoheQy\nCImOAjAR0um4cPND/Z0jh+gr2Xru6CgEzVW73Xzx8msfzn2cCEkAHQWhuo10fHPS6mHuFkdC\nmoyOwtDd2fC2k7UrtoQ0ER0FohrS5v3bje1nn9/TmJDeoqNQlHc2rN9PYElIk9BRMKohzSaf\ns0pI79BROCYOyCoOkXJIdBSQSkiL1ZCJeD4gG/oV94aOQlK7Qrb/RDrfLPkGS+mGREdBqV0h\nK4SQXqCjsFRCOi7mu+FTGzRE7+cnGhIdBWbjXLvpQ/R+fpoh0VFoRkLareur0RerTxZdhPQM\nHQVnYvf3cdZI7f19HQjpCToKz3tIfdbmVq742FePDtvCvd1XTkhddGSAiZAKt7893r8/H4+Q\nOujIAhMhtb6HA7LD0JEJJkJiiTQeHdlgIqTzNtK2fsMXtpEGoiMjFELqcYbPvPEds7dnQRBS\nCx1ZYSOk025VHUcqFmuOIw1AR2aYWLWbOMSg56cUEh3ZQUjxoiNDCCladGQJIcWKjkwhpEjR\nkS0mTlpVHCKVkOjIGEKKEh1ZQ0gxoiNzCClCdGQPIcWHjgwipOjQkUWEFBs6MomQIkNHNhFS\nXOjIKEKKCh1ZRUgxoSOzCCkidGQXIcWDjgwjpGjQkWWEFAs6Mo2QIkFHthFSHOjIOEKKAh1Z\nR0gxoCPzCCkCdGQfIdlHRxEgJPPoKAaEZB0dRYGQjKOjOBCSbXQUCUIyjY5iQUiW0VE0CMkw\nOooHIdlFRxEhJLPoKCaEZBUdRYWQjKKjuBCSTXQUGUIyiY5iQ0gW0VF0CMkgOooPIdlDRxEi\nJHPoKEaEZA0dRYmQjKGjOBGSLXQUKUIyhY5iRUiW0FG0CMkQOooXIdlBRxEjJDPoKGaEZAUd\nRY2QjKCjuBGSDXQUOUIygY5iR0gW0FH0CMkAOoofIYVHRwkgpODoKAWEFBodJYGQAqOjNBBS\nWHSUCEIKio5SQUgh0VEyCCkgOkoHIYVDRwkhpGDoKCWEFAodJYWQAqGjtBBSGHSUGEIKgo5S\nQ0gh0FFyCCkAOkoPIemjowQRkjo6ShEhaaOjJBGSMjpKEyHpoqNEEZIqOkoVIWmio2QRkiI6\nShch6aGjhBGSGjpKGSFpoaOkEZISOkobIemgo8QRkgo6Sh0haaCj5BGSAjpKHyH5R0cZICTv\n6CgHhOQbHWWBkDyjozwQkl90lAlC8oqOckFIPtFRNgjJIzrKByH5Q0cZISRv6CgnhOQLHWWF\nkDyho7wQkh90lBkbIR2Xzs23l4m8nUokIdFRbkyEdCxcaXGZ1X0McXu+Skh0lB0TIa3c5lzT\nppjXs7qPIW7P1wiJjvJjIqSifuKhmB1SCImOMmQipGs7x/k8gZDoKEcmQpq54/XRPPqQ6ChL\nJkLauOXl0cHNIw+JjvJkIqTT6lbP1sUdEh1lykZIp/3i+uiwjDkkOsqVkZDUhvAbEh1li5AE\n0VG+CEkOHWXMXkjR7mygo5zFEJJrmjx1X68kHWXNXkh+h/AWEh3ljZBk0FHmMgrpsm7o5WWk\no9wZCWm3XtSXJK12voZwHkOio+yZCOk4a+xNmHsZgv118MpESCtXfOyrR4dt4VY+hmB/Hbwy\nEVLh9rfHe1f4GIL9dfDKREito0O+Dsiyvw4emQgp5iUSHaFkIqTzNtL2UD2KbhuJjlAxEdJp\n3thrNzu++05jIdERajZCOu1W1XGkYrH2eBxJ/tWjI1wYCUlhCA8h0RGuCGk8OsINIY1GR7gj\npLHoCA2ENBIdoYmQxqEjtBDSKHSENkIag47wgJBGoCM8IqTh6AgdhDQYHaGLkIaiIzxBSAPR\nEZ4hpGHoCE8R0iB0hOcIaQg6wguENAAd4RVC6o+O8BIh9UZHeI2Q+qIjvEFIPdER3iGkfugI\nbxFSL3SE9wipDzrCJwipBzrCZwjpc3SETyUZkntu5EtER/hciiG96GhkSHSEHtIMSfAFoiP0\nQUjv0RF6IaS36Aj9ENI7dISeCOkNOkJfhPQaHaE3QnqJjtAfIb1CRxiAkF6gIwxBSM/REQYh\npKfoCMMQ0jN0hIEI6Qk6wlCE1EVHGIyQOugIwyUTksQlfBU6wgiphCRyLWyJjjBGOiHJvB50\nhFEIqYWOMA4hNdERRiKkBjrCWIR0R0cYjZBu6AjjEdIVHWECQrqgI0xBSDU6wiSEVKEjTENI\nJTrCRIT0Hx1hOkKiIwggJDqCAEKiIwjIPiQ6goTcQ6IjiMg8JDqCjLxDoiMIyTokOoKUnEOi\nI4jJOCQ6gpx8Q6IjCMo2JDqCpFxDoiOIyjQkOoKsPEOiIwjLMiQ6grQcQ6IjiMswJDqCvPxC\noiN4kF1IdAQfcguJjuBFZiHREfzIKyQ6gidZhURH8CWnkOgI3mQUEh3Bn3xCoiN4lE1IdASf\ncgmJjuBVJiHREfzKIyQ6gmdZhERH8C2HkOgI3mUQEh3Bv/RDoiMoSD4kOoKG1EOiI6hIPCQ6\ngo60Q6IjKEk6JDqClpRDoiOoSTgkOoKedEOiIyhKNiQ6gqZUQ6IjqDIS0m69cKXFajduiIeQ\n6Ai6TIR0nLm7+agh2iHREZSZCGnlio999eiwLdxqzBCtkOgI2kyEVLj97fHeFWOGaIZER1Bn\nIiTnXn3Qe4hGSHQEfSZCEl0i0RECMBHSeRtpe6geTd9GoiOEYCKk07yx1252HDPENSQ6QhA2\nQjrtVtVxpGKxnnYciY4QhpGQJg9Rh0RHCCSpkOgIoaQUEh0hGHshjT6OREcIJ4aQXNPLZ/10\nQDiSVQRctTv99D82MEJcIdERjIoqJDqCVTFd2EdHMCuiC/voCHbFc2EfHcGwaC6joCNYFsuF\nfXQE0yJZItERbIvjwj46gnFRXNhHR7Auhgv76AjmRXBmAx3BPvsh0REiYD4kOkIMrIdER4iC\n8ZDoCHGwHRIdIRKmQ6IjxMJySHSEaBgOiY4QD7sh0REiYjYkOkJMrIZER4iK0ZDoCHGxGRL3\nU0Vshs/lCiGZHJvxGV90fEJifMa3NrGIxmZ8xickxmd8a+MTEuMzvrWJRTQ24zM+ITE+41sb\nn5AYn/GtTSyisRmf8QmJ8Rnf2viExPiMb21iEY3N+IyfTEhAMggJEEBIgABCAgQQEiCAkAAB\nhAQIICRAACEBAggJEEBIgABCAgQQEiCAkAABhAQIICRAQICQNu0xV4UrVke10R+HOwYe/7Rf\nOrc8hBv/bKc4F3TG38zCvv5Cv3/9kPbte/3Pq7v/z7RGfxzuUFSfKLTm5M7/7rYeX2tOevZy\nHwu9uaAz/irw/7/U/Kce0r5ohbRzxb783E5n9M5wS7c6lb/NZaDxT8X5E8dF9VMEGf9sMeZt\nTITG37vlsVxJCfX6i81/2iFt3Lz1a1u57fnPD7fWGb4z3OWH0ZqVOuN/VAkdXRFo/OojvZA6\n4y8Cv/5i8592SOf5pvWiLVy5UrV3C53hO8Nd1mq0ZuTO+Eu31xn5xfjnlduHf9q0x69o/QSd\n8cXmP+2Q9g8vmvISoTPc+rJqp7RE7Iw/c6d1Ua3ehBm/3Eg46IX04td9dPNA44vNfwH22pkK\n6bQp9zYUG53hn/0iF9XGdqjxz/+SfKi9+i9/3ZtqBSvE+IQ0aezWjFRSWiA9+0WWOxuWwZaI\n1UpN6JAOhdKaPSEJj30fblOu2p1nZKVF0pNfZLmNdNDa/99dtSx3PAcO6VgordilHFKhG1Jn\nuJkrN0+OWjNyZ3zlf0gex19W61R6IT39dc/VjiJ2xxeb/0KHVO81OejutWsMpzwjd8ZX3v37\nOL67CTN++cFsrndex5PXX2j+Cx3Suvoncat1QLIzXP0vktpxnM749ScOWnutHsfXDqn7695q\n/a8/H19s/gsdUugzG1auPM9qFezMgvPW0bHcRvsINH4l4JkNav+EvBg/2jMbTvdfW/33rPr3\nUO3VbAxXjz8PPP468PjtR+rjL3WXiN3/f6n5L3hI9dnXaoM3hrv8HKHH387Djn9SDelxfOVV\ny+7/v9T8FyAkID2EBAggJEAAIQECCAkQQEiAAEICBBASIICQAAGEBAggJEAAIQECCAkQQEiA\nAEICBBASIICQAAGEBAggJEAAIQECCAkQQEhKnt8u57O759RPKV6/x2w1gW2PSV1Gn3du4Kb0\nPhCpIyQlU0J68x635QRmrsekbuM/lDRjDhDBy6jk+Xz+eUjln8f52zvB9rop3PUufo/3QlS8\np13SeBmVTAnpk3uTDwmp882EJIOXUUlzht0u3OXunvUmzvy87VJvqmxm7bwqgb4AAAJxSURB\nVHcPbM/+56/O6q/ennL+wmWF0bnrm9NUb1Xzfkq3n+C2tvn4/RiGkJR03iTQrS6f3dQflnPx\n4vFG1K0l0v025fenNEMq3w72dLkv/asp1at295/gGlLn+zEMISlp7Gtw5XtPfFwelu8ssy8/\nnFVvcXIsN4i2jWeVfx6qbaSPyxsnfDSfckmo/tb6Xe7Ldyp5NqWLffcneDYyhiEkJZ2ddrfZ\n2N1m38Xl7QMXnWeVb1C5uLyVz7z5lFZIp2rdrtwP93JK833zR7r+8eT7MQwhKWlt1B+26/lt\nNl45t9jv6+/p1nY/jtR4c8HmU5ohLc/rdofbGlu321mxffYT6L8pRHp45ZQ059F5Yy3v/Me6\nuBwpejH7tx8/PqUZ0u68brcqjxS9mNLOucOzn4CQJuOVU9KYR5duttkeGrPxabuaXTd4Xj7r\n4e1uG0+5h3QqZuV/r6e0qFfeOj8BBU3FC6jkceHQCunyaNHZ2G/O4NdtpNYbST+EtHKbaofD\nqyntrzsbHn6C7vdjGEJS0gppd9rft1Bm9S602WW/3GnT2kVwn0Jjr939KXVIt02ocxzVboOX\nU6oXSa2f4PD0+zEMISlpJLG6bI/s6s9+3D66bLo0TqxrrXLdjyN9tCYwc+VhpvpbZ5djQa+m\ndKwWSY2foH5y9/sxDCEpaSaxLM/CrtbR7mc21OeSbs7zdfNU7/a2y6Zondmwu3zDbnYP6eO6\njvZqSqtqqXP/Ceond78fwxASIICQAAGEBAggJEAAIQECCAkQQEiAAEICBBASIICQAAGEBAgg\nJEAAIQECCAkQQEiAAEICBBASIICQAAGEBAj4P4z5y9wRprZ9AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusionMatrix(ptest$predictions,testnorm$Class)\n",
    "roccurve<-roc(testnorm$Class~ptest$decisionValues[,1])\n",
    "plot(roccurve,colorize=TRUE,xlab=\"False Positive Rate\",ylab=\"True Positive Rate\",auc.polygon = TRUE,print.auc=TRUE)\n",
    "\n",
    "# pd <- prediction(ptrain$decisionValues[,1],trainnorm$Class) \n",
    "# per <- performance(pd,\"tpr\",\"fpr\")\n",
    "# plot(per, lty=3, col=\"red\",colorize=TRUE)\n",
    "\n",
    "# data(ROCR.hiv)\n",
    "# attach(ROCR.hiv)\n",
    "# pred.svm <- prediction(hiv.svm$predictions, hiv.svm$labels)\n",
    "# perf.svm <- performance(pred.svm, 'tpr', 'fpr') \n",
    "# pred.nn <- prediction(hiv.nn$predictions, hiv.svm$labels) \n",
    "# perf.nn <- performance(pred.nn, 'tpr', 'fpr')\n",
    "# plot(perf.svm, lty=3, col=\"red\",main=\"SVMs and NNs for prediction of HIV-1 coreceptor usage\")\n",
    "# plot(perf.nn, lty=3, col=\"blue\",add=TRUE) \n",
    "# plot(perf.svm, avg=\"vertical\", lwd=3, col=\"red\", spread.estimate=\"stderror\",plotCI.lwd=2,add=TRUE) \n",
    "# plot(perf.nn, avg=\"vertical\", lwd=3, col=\"blue\", spread.estimate=\"stderror\",plotCI.lwd=2,add=TRUE) \n",
    "# legend(0.6,0.6,c('SVM','NN'),col=c('red','blue'),lwd=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 178   5\n",
      "[1] 570   5\n",
      "[1] 598   5\n",
      "[1] 142   5\n",
      "[1] 456   5\n",
      "[1] 299   5\n",
      "[1] 299   5\n"
     ]
    }
   ],
   "source": [
    "#1_bii\n",
    "alldata= read.csv(\"transfusion.data\")\n",
    "\n",
    "colnames(alldata) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "\n",
    "salldata<-scale(alldata[,1:4],center=TRUE,scale=TRUE)\n",
    "alldatanorm=cbind(salldata,alldata$Class)\n",
    "colnames(alldatanorm) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "alldatanorm=as.data.frame(alldatanorm)\n",
    "alldatanorm$Class=as.factor(alldatanorm$Class)\n",
    "ones=which(alldatanorm$Class==1)\n",
    "zeros=which(alldatanorm$Class==0)\n",
    "\n",
    "newclassone<-alldatanorm[ones,]\n",
    "newclasszero<-alldatanorm[zeros,]\n",
    "\n",
    "zeroind=ceiling(1/5*nrow(newclasszero))\n",
    "oneind=ceiling(1/5*nrow(newclassone))\n",
    "\n",
    "testnorm = rbind(newclasszero[1:zeroind,],newclassone[1:oneind,])\n",
    "trainnorm=rbind(newclasszero[(zeroind+1):nrow(newclasszero),],newclassone[(oneind+1):nrow(newclassone),])\n",
    "rownames(testnorm) <- seq(length=nrow(testnorm))\n",
    "rownames(trainnorm) <- seq(length=nrow(trainnorm))\n",
    "\n",
    "\n",
    "ones=which(trainnorm$Class==1)\n",
    "zeros=which(trainnorm$Class==0)\n",
    "trainone<-trainnorm[ones,]\n",
    "trainzero<-trainnorm[zeros,]\n",
    "smp_size_zero <- floor(0.5 * nrow(trainzero))\n",
    "smp_size_one <- floor(0.5*nrow(trainone))\n",
    "\n",
    "labeldata=rbind(trainzero[sample(nrow(trainzero), size = smp_size_zero),],trainone[sample(nrow(trainone),size=smp_size_one),])\n",
    "unlabeldata=rbind(trainzero[-sample(nrow(trainzero), size = smp_size_zero),],trainone[-sample(nrow(trainone),size=smp_size_one),])\n",
    "rownames(labeldata) <- seq(length=nrow(labeldata))\n",
    "rownames(unlabeldata) <- seq(length=nrow(unlabeldata))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# labeldata$dist = 0\n",
    "# unlabeldata$dist = 0\n",
    "# print(dim(alldatanorm))\n",
    "print(dim(newclassone))\n",
    "print(dim(newclasszero))\n",
    "print(dim(trainnorm))\n",
    "print(dim(trainone))\n",
    "print(dim(trainzero))\n",
    "print(dim(labeldata))\n",
    "print(dim(unlabeldata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$TypeDetail\n",
       "[1] \"L1-regularized L2-loss support vector classification (L1R_L2LOSS_SVC)\"\n",
       "\n",
       "$Type\n",
       "[1] 5\n",
       "\n",
       "$W\n",
       "             R          F           M         T      Bias\n",
       "[1,] 0.1728406 -0.3155432 -0.05451056 0.2407457 0.7055171\n",
       "\n",
       "$Bias\n",
       "[1] 1\n",
       "\n",
       "$ClassNames\n",
       "[1] 0 1\n",
       "Levels: 0 1\n",
       "\n",
       "$NbClass\n",
       "[1] 2\n",
       "\n",
       "attr(,\"class\")\n",
       "[1] \"LiblineaR\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#semisupervised\n",
    "alldata= read.csv(\"transfusion.data\")\n",
    "\n",
    "colnames(alldata) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "\n",
    "salldata<-scale(alldata[,1:4],center=TRUE,scale=TRUE)\n",
    "alldatanorm=cbind(salldata,alldata$Class)\n",
    "colnames(alldatanorm) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "alldatanorm=as.data.frame(alldatanorm)\n",
    "alldatanorm$Class=as.factor(alldatanorm$Class)\n",
    "ones=which(alldatanorm$Class==1)\n",
    "zeros=which(alldatanorm$Class==0)\n",
    "\n",
    "newclassone<-alldatanorm[ones,]\n",
    "newclasszero<-alldatanorm[zeros,]\n",
    "\n",
    "zeroind=ceiling(1/5*nrow(newclasszero))\n",
    "oneind=ceiling(1/5*nrow(newclassone))\n",
    "\n",
    "testnorm = rbind(newclasszero[1:zeroind,],newclassone[1:oneind,])\n",
    "trainnorm=rbind(newclasszero[(zeroind+1):nrow(newclasszero),],newclassone[(oneind+1):nrow(newclassone),])\n",
    "rownames(testnorm) <- seq(length=nrow(testnorm))\n",
    "rownames(trainnorm) <- seq(length=nrow(trainnorm))\n",
    "\n",
    "\n",
    "ones=which(trainnorm$Class==1)\n",
    "zeros=which(trainnorm$Class==0)\n",
    "trainone<-trainnorm[ones,]\n",
    "trainzero<-trainnorm[zeros,]\n",
    "smp_size_zero <- floor(0.5 * nrow(trainzero))\n",
    "smp_size_one <- floor(0.5*nrow(trainone))\n",
    "\n",
    "labeldata=rbind(trainzero[sample(nrow(trainzero), size = smp_size_zero),],trainone[sample(nrow(trainone),size=smp_size_one),])\n",
    "unlabeldata=rbind(trainzero[-sample(nrow(trainzero), size = smp_size_zero),],trainone[-sample(nrow(trainone),size=smp_size_one),])\n",
    "rownames(labeldata) <- seq(length=nrow(labeldata))\n",
    "rownames(unlabeldata) <- seq(length=nrow(unlabeldata))\n",
    "\n",
    "\n",
    "\n",
    "labeldata$dist = 0\n",
    "unlabeldata$dist = 0\n",
    "\n",
    "\n",
    "while(nrow(unlabeldata)>0){\n",
    "    labeldata=as.data.frame(labeldata)\n",
    "    tryCosts= c(1000,100,10,1,0.1,0.01,0.001)\n",
    "    bestCo= NA\n",
    "    bestAcc= 0\n",
    "\n",
    "    for(co in tryCosts){\n",
    "        acc=LiblineaR(data=labeldata[,1:4],target=labeldata$Class,type=5,cost=co,bias=1,cross=5,verbose=FALSE)\n",
    "# cat(\"Results for C=\",co,\" : \",1-acc,\" error.\\n\",sep=\"\")\n",
    "        if(acc>bestAcc){\n",
    "            bestCost=co\n",
    "            bestAcc=acc\n",
    "        }\n",
    "    }\n",
    "\n",
    "    model =LiblineaR(data=labeldata[,1:4],target=labeldata$Class,type=5,cost=bestCost,bias=1,verbose=FALSE)\n",
    "    y=model$W[1:4]\n",
    "    z=model$W[5]\n",
    "    unlabeldata$dist<-apply(unlabeldata,1,function(x) abs(sum(as.numeric(x[1:4])*y)+z))\n",
    "    a= which(unlabeldata$dist==(min(unlabeldata$dist)))\n",
    "    if (length(a)!=1){\n",
    "            a = a[1]\n",
    "        }\n",
    "    unlabeldata[a,]$Class = predict(model,unlabeldata[a,1:4])$prediction\n",
    "    labeldata=rbind(labeldata,unlabeldata[a,])\n",
    "    unlabeldata=unlabeldata[-a,]\n",
    "\n",
    "}\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptest=predict(model,newx=testnorm[,1:4],decisionValues=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 111  31\n",
       "         1   3   5\n",
       "                                          \n",
       "               Accuracy : 0.7733          \n",
       "                 95% CI : (0.6979, 0.8376)\n",
       "    No Information Rate : 0.76            \n",
       "    P-Value [Acc > NIR] : 0.3931          \n",
       "                                          \n",
       "                  Kappa : 0.1534          \n",
       " Mcnemar's Test P-Value : 3.649e-06       \n",
       "                                          \n",
       "            Sensitivity : 0.9737          \n",
       "            Specificity : 0.1389          \n",
       "         Pos Pred Value : 0.7817          \n",
       "         Neg Pred Value : 0.6250          \n",
       "             Prevalence : 0.7600          \n",
       "         Detection Rate : 0.7400          \n",
       "   Detection Prevalence : 0.9467          \n",
       "      Balanced Accuracy : 0.5563          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAXVBMVEUAAABCQkJNTU1aWlpo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\noP8DonUdNmEGF84AAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "confusionMatrix(ptest$predictions,testnorm$Class)\n",
    "roccurve<-roc(testnorm$Class~ptest$decisionValues[,1])\n",
    "plot(roccurve,colorize=TRUE,xlab=\"False Positive Rate\",ylab=\"True Positive Rate\",auc.polygon = TRUE,print.auc=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata= read.csv(\"transfusion.data\")\n",
    "\n",
    "colnames(alldata) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "\n",
    "salldata<-scale(alldata[,1:4],center=TRUE,scale=TRUE)\n",
    "alldatanorm=cbind(salldata,alldata$Class)\n",
    "colnames(alldatanorm) <-c(\"R\",\"F\",\"M\",\"T\",\"Class\")\n",
    "alldatanorm=as.data.frame(alldatanorm)\n",
    "alldatanorm$Class=as.factor(alldatanorm$Class)\n",
    "ones=which(alldatanorm$Class==1)\n",
    "zeros=which(alldatanorm$Class==0)\n",
    "\n",
    "newclassone<-alldatanorm[ones,]\n",
    "newclasszero<-alldatanorm[zeros,]\n",
    "\n",
    "# ones=which(alldata$Class==1)\n",
    "# zeros=which(alldata$Class==0)\n",
    "\n",
    "# newclassone<-alldata[ones,]\n",
    "# newclasszero<-alldata[zeros,]\n",
    "\n",
    "\n",
    "\n",
    "zeroind=ceiling(1/5*nrow(newclasszero))\n",
    "oneind=ceiling(1/5*nrow(newclassone))\n",
    "\n",
    "testnorm = rbind(newclasszero[1:zeroind,],newclassone[1:oneind,])\n",
    "trainnorm=rbind(newclasszero[(zeroind+1):nrow(newclasszero),],newclassone[(oneind+1):nrow(newclassone),])\n",
    "rownames(testnorm) <- seq(length=nrow(testnorm))\n",
    "rownames(trainnorm) <- seq(length=nrow(trainnorm))\n",
    "# nolabeltrain = trainnorm[,1:4]\n",
    "# nolabeltest = testnorm[,1:4]\n",
    "# trainlabel=trainnorm[,5]\n",
    "# testlabel=testnorm[,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>R</th><th scope=col>F</th><th scope=col>M</th><th scope=col>T</th><th scope=col>Class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>-1.0508053 </td><td> 3.1656657 </td><td> 3.1656657 </td><td> 1.75240657</td><td>0          </td></tr>\n",
       "\t<tr><td>-0.6802243 </td><td>-0.2593982 </td><td>-0.2593982 </td><td>-1.24225460</td><td>0          </td></tr>\n",
       "\t<tr><td>-1.0508053 </td><td> 1.1106273 </td><td> 1.1106273 </td><td> 0.02945083</td><td>0          </td></tr>\n",
       "\t<tr><td>-0.6802243 </td><td> 2.9944125 </td><td> 2.9944125 </td><td> 0.97297421</td><td>0          </td></tr>\n",
       "\t<tr><td>-1.1743323 </td><td>-0.4306514 </td><td>-0.4306514 </td><td>-1.24225460</td><td>0          </td></tr>\n",
       "\t<tr><td>-1.0508053 </td><td> 1.2818805 </td><td> 1.2818805 </td><td> 0.52172390</td><td>0          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " R & F & M & T & Class\\\\\n",
       "\\hline\n",
       "\t -1.0508053  &  3.1656657  &  3.1656657  &  1.75240657 & 0          \\\\\n",
       "\t -0.6802243  & -0.2593982  & -0.2593982  & -1.24225460 & 0          \\\\\n",
       "\t -1.0508053  &  1.1106273  &  1.1106273  &  0.02945083 & 0          \\\\\n",
       "\t -0.6802243  &  2.9944125  &  2.9944125  &  0.97297421 & 0          \\\\\n",
       "\t -1.1743323  & -0.4306514  & -0.4306514  & -1.24225460 & 0          \\\\\n",
       "\t -1.0508053  &  1.2818805  &  1.2818805  &  0.52172390 & 0          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "R | F | M | T | Class | \n",
       "|---|---|---|---|---|---|\n",
       "| -1.0508053  |  3.1656657  |  3.1656657  |  1.75240657 | 0           | \n",
       "| -0.6802243  | -0.2593982  | -0.2593982  | -1.24225460 | 0           | \n",
       "| -1.0508053  |  1.1106273  |  1.1106273  |  0.02945083 | 0           | \n",
       "| -0.6802243  |  2.9944125  |  2.9944125  |  0.97297421 | 0           | \n",
       "| -1.1743323  | -0.4306514  | -0.4306514  | -1.24225460 | 0           | \n",
       "| -1.0508053  |  1.2818805  |  1.2818805  |  0.52172390 | 0           | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  R          F          M          T           Class\n",
       "1 -1.0508053  3.1656657  3.1656657  1.75240657 0    \n",
       "2 -0.6802243 -0.2593982 -0.2593982 -1.24225460 0    \n",
       "3 -1.0508053  1.1106273  1.1106273  0.02945083 0    \n",
       "4 -0.6802243  2.9944125  2.9944125  0.97297421 0    \n",
       "5 -1.1743323 -0.4306514 -0.4306514 -1.24225460 0    \n",
       "6 -1.0508053  1.2818805  1.2818805  0.52172390 0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(testnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means clustering with 2 clusters of sizes 469, 129\n",
      "\n",
      "Cluster means:\n",
      "             R          F          M          T\n",
      "1  0.256368577 -0.4343029 -0.4343029 -0.2857859\n",
      "2 -0.006092371  1.1531087  1.1531087  1.2496393\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  2   1   2   1   1   1   2   1   2   2   2   1   1   2   1   1   1   1   1   1 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1   1   2 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  2   1   1   2   2   1   1   1   2   2   1   1   1   1   2   1   1   1   1   2 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  2   1   2   1   1   1   1   1   2   1   2   2   2   1   1   1   1   1   2   1 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   2   1 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  1   1   1   1   1   2   1   1   1   1   2   1   1   1   1   1   2   1   1   2 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  1   1   1   2   1   1   1   1   1   2   1   2   1   1   1   1   1   1   1   1 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  1   1   1   1   1   1   2   1   1   2   2   1   1   1   1   2   1   1   1   2 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  1   1   1   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  2   1   1   1   1   1   1   1   2   2   2   2   2   1   1   1   2   2   1   1 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   2   1 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  1   1   1   2   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  1   2   1   1   1   1   1   2   2   2   1   1   1   1   1   1   1   1   1   1 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  2   1   1   1   2   1   1   2   2   1   1   2   1   2   1   1   1   1   1   1 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  1   1   1   1   2   1   1   1   1   1   2   1   2   1   1   1   1   1   2   1 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  1   1   1   1   1   2   2   2   1   1   2   2   1   1   1   1   1   1   2   1 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  1   1   2   1   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  2   1   1   1   1   1   1   1   2   1   1   1   2   1   2   1   2   1   1   1 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  1   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  1   1   1   1   1   1   2   1   2   1   1   2   1   1   1   1   1   1   1   1 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  2   1   2   1   1   2   1   1   1   1   1   1   1   1   2   1   1   1   2   2 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  2   2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   1   1   1 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  1   1   1   1   1   1   2   1   1   2   1   1   1   2   2   1   1   1   1   1 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  1   1   2   1   1   1   1   1   2   2   1   1   1   2   2   1   1   1 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 879.6465 561.9629\n",
      " (between_SS / total_SS =  34.4 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 431, 167\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "2 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  2   1   2   1   2   1   2   2   2   2   2   1   1   2   1   1   1   1   1   1 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1   1   2 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  2   1   1   2   2   1   1   1   2   2   1   1   1   1   2   1   1   2   1   2 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  2   1   2   1   1   2   2   1   2   1   2   2   2   1   1   1   1   1   2   1 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  1   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   2   1 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  1   1   1   1   1   2   1   1   1   1   2   1   2   1   1   1   2   1   1   2 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  2   2   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  1   1   1   2   1   1   1   1   1   2   1   2   1   1   2   1   2   1   1   1 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  1   1   1   1   1   1   2   1   1   2   2   1   2   1   2   2   1   1   1   2 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  1   1   2   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  2   2   2   1   1   1   1   1   2   2   2   2   2   1   2   1   2   2   1   1 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   2   2 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  2   1   1   2   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  1   2   1   2   1   1   1   2   2   2   1   1   1   1   1   1   1   1   1   1 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  2   1   1   1   2   1   2   2   2   1   1   2   1   2   2   1   1   1   1   1 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  1   1   1   1   2   1   1   1   1   1   2   1   2   1   2   1   1   1   2   1 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  1   1   1   2   1   2   2   2   1   1   2   2   1   1   1   1   1   1   2   1 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   2   1 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  1   1   2   1   2   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  2   1   1   1   1   1   1   1   1   2   2   1   1   1   1   1   2   1   1   1 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  2   1   1   1   1   1   1   1   2   1   1   2   2   1   2   2   2   1   1   1 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  1   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2   1 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  1   1   1   1   1   1   2   1   2   2   1   2   1   1   1   1   1   1   1   1 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  2   1   2   1   1   2   1   1   1   1   1   1   1   1   2   1   1   1   2   2 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  2   2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   1   1   1 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  1   1   1   2   1   1   2   1   1   2   1   1   1   2   2   1   1   1   1   1 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  1   1   2   1   1   1   1   1   2   2   1   1   2   2   2   1   1   1 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 742.4645 695.8635\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 431, 167\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "2 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  2   1   2   1   2   1   2   2   2   2   2   1   1   2   1   1   1   1   1   1 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1   1   2 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  2   1   1   2   2   1   1   1   2   2   1   1   1   1   2   1   1   2   1   2 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  2   1   2   1   1   2   2   1   2   1   2   2   2   1   1   1   1   1   2   1 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  1   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   2   1 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  1   1   1   1   1   2   1   1   1   1   2   1   2   1   1   1   2   1   1   2 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  2   2   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  1   1   1   2   1   1   1   1   1   2   1   2   1   1   2   1   2   1   1   1 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  1   1   1   1   1   1   2   1   1   2   2   1   2   1   2   2   1   1   1   2 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  1   1   2   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  2   2   2   1   1   1   1   1   2   2   2   2   2   1   2   1   2   2   1   1 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   2   2 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  2   1   1   2   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  1   2   1   2   1   1   1   2   2   2   1   1   1   1   1   1   1   1   1   1 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  2   1   1   1   2   1   2   2   2   1   1   2   1   2   2   1   1   1   1   1 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  1   1   1   1   2   1   1   1   1   1   2   1   2   1   2   1   1   1   2   1 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  1   1   1   2   1   2   2   2   1   1   2   2   1   1   1   1   1   1   2   1 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   2   1 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  1   1   2   1   2   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  2   1   1   1   1   1   1   1   1   2   2   1   1   1   1   1   2   1   1   1 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  2   1   1   1   1   1   1   1   2   1   1   2   2   1   2   2   2   1   1   1 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  1   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2   1 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  1   1   1   1   1   1   2   1   2   2   1   2   1   1   1   1   1   1   1   1 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  2   1   2   1   1   2   1   1   1   1   1   1   1   1   2   1   1   1   2   2 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  2   2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   1   1   1 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  1   1   1   2   1   1   2   1   1   2   1   1   1   2   2   1   1   1   1   1 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  1   1   2   1   1   1   1   1   2   2   1   1   2   2   2   1   1   1 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 742.4645 695.8635\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 167, 431\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "2  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  1   2   1   2   1   2   1   1   1   1   1   2   2   1   2   2   2   2   2   2 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   1   2   2   2   1 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  1   2   2   1   1   2   2   2   1   1   2   2   2   2   1   2   2   1   2   1 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  1   2   1   2   2   1   1   2   1   2   1   1   1   2   2   2   2   2   1   2 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  2   2   1   2   2   2   2   2   2   2   2   2   1   2   2   2   2   2   1   2 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  2   2   2   2   2   1   2   2   2   2   1   2   1   2   2   2   1   2   2   1 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  1   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   2 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  1   1   1   1   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   1 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  2   2   2   1   2   2   2   2   2   1   2   1   2   2   1   2   1   2   2   2 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   1 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  2   2   2   2   2   2   1   2   2   1   1   2   1   2   1   1   2   2   2   1 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  2   2   1   2   1   2   2   2   2   2   2   2   2   2   1   2   2   2   2   2 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  1   1   1   2   2   2   2   2   1   1   1   1   1   2   1   2   1   1   2   2 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  2   1   1   1   2   2   2   2   2   2   2   2   1   1   1   1   1   2   1   1 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  1   2   2   1   2   2   2   2   2   2   2   2   2   2   2   1   2   1   2   2 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  2   1   2   1   2   2   2   1   1   1   2   2   2   2   2   2   2   2   2   2 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  1   2   2   2   1   2   1   1   1   2   2   1   2   1   1   2   2   2   2   2 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  2   2   2   2   1   2   2   2   2   2   1   2   1   2   1   2   2   2   1   2 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  2   2   2   1   2   1   1   1   2   2   1   1   2   2   2   2   2   2   1   2 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  2   2   2   2   2   2   2   2   2   1   2   2   2   2   2   2   2   2   1   2 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  2   2   1   2   1   2   2   2   1   2   2   2   2   2   2   2   2   2   2   2 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  1   2   2   2   2   2   2   2   2   1   1   2   2   2   2   2   1   2   2   2 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  1   2   2   2   2   2   2   2   1   2   2   1   1   2   1   1   1   2   2   2 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  2   1   1   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   1   2 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  2   2   2   2   2   2   1   2   1   1   2   1   2   2   2   2   2   2   2   2 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  1   2   1   2   2   1   2   2   2   2   2   2   2   2   1   2   2   2   1   1 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  1   1   1   1   1   1   2   1   2   1   1   1   1   1   1   1   2   2   2   2 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  2   2   2   1   2   2   1   2   2   1   2   2   2   1   1   2   2   2   2   2 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  2   2   1   2   2   2   2   2   1   1   2   2   1   1   1   2   2   2 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 695.8635 742.4645\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 431, 167\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "2 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  2   1   2   1   2   1   2   2   2   2   2   1   1   2   1   1   1   1   1   1 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1   1   2 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  2   1   1   2   2   1   1   1   2   2   1   1   1   1   2   1   1   2   1   2 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  2   1   2   1   1   2   2   1   2   1   2   2   2   1   1   1   1   1   2   1 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  1   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   2   1 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  1   1   1   1   1   2   1   1   1   1   2   1   2   1   1   1   2   1   1   2 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  2   2   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  1   1   1   2   1   1   1   1   1   2   1   2   1   1   2   1   2   1   1   1 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  1   1   1   1   1   1   2   1   1   2   2   1   2   1   2   2   1   1   1   2 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  1   1   2   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  2   2   2   1   1   1   1   1   2   2   2   2   2   1   2   1   2   2   1   1 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   2   2 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  2   1   1   2   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  1   2   1   2   1   1   1   2   2   2   1   1   1   1   1   1   1   1   1   1 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  2   1   1   1   2   1   2   2   2   1   1   2   1   2   2   1   1   1   1   1 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  1   1   1   1   2   1   1   1   1   1   2   1   2   1   2   1   1   1   2   1 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  1   1   1   2   1   2   2   2   1   1   2   2   1   1   1   1   1   1   2   1 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   2   1 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  1   1   2   1   2   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  2   1   1   1   1   1   1   1   1   2   2   1   1   1   1   1   2   1   1   1 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  2   1   1   1   1   1   1   1   2   1   1   2   2   1   2   2   2   1   1   1 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  1   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2   1 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  1   1   1   1   1   1   2   1   2   2   1   2   1   1   1   1   1   1   1   1 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  2   1   2   1   1   2   1   1   1   1   1   1   1   1   2   1   1   1   2   2 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  2   2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   1   1   1 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  1   1   1   2   1   1   2   1   1   2   1   1   1   2   2   1   1   1   1   1 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  1   1   2   1   1   1   1   1   2   2   1   1   2   2   2   1   1   1 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 742.4645 695.8635\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 431, 167\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "2 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  2   1   2   1   2   1   2   2   2   2   2   1   1   2   1   1   1   1   1   1 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1   1   2 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  2   1   1   2   2   1   1   1   2   2   1   1   1   1   2   1   1   2   1   2 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  2   1   2   1   1   2   2   1   2   1   2   2   2   1   1   1   1   1   2   1 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  1   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   2   1 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  1   1   1   1   1   2   1   1   1   1   2   1   2   1   1   1   2   1   1   2 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  2   2   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  1   1   1   2   1   1   1   1   1   2   1   2   1   1   2   1   2   1   1   1 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  1   1   1   1   1   1   2   1   1   2   2   1   2   1   2   2   1   1   1   2 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  1   1   2   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  2   2   2   1   1   1   1   1   2   2   2   2   2   1   2   1   2   2   1   1 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   2   2 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  2   1   1   2   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  1   2   1   2   1   1   1   2   2   2   1   1   1   1   1   1   1   1   1   1 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  2   1   1   1   2   1   2   2   2   1   1   2   1   2   2   1   1   1   1   1 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  1   1   1   1   2   1   1   1   1   1   2   1   2   1   2   1   1   1   2   1 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  1   1   1   2   1   2   2   2   1   1   2   2   1   1   1   1   1   1   2   1 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   2   1 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  1   1   2   1   2   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  2   1   1   1   1   1   1   1   1   2   2   1   1   1   1   1   2   1   1   1 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  2   1   1   1   1   1   1   1   2   1   1   2   2   1   2   2   2   1   1   1 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  1   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2   1 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  1   1   1   1   1   1   2   1   2   2   1   2   1   1   1   1   1   1   1   1 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  2   1   2   1   1   2   1   1   1   1   1   1   1   1   2   1   1   1   2   2 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  2   2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   1   1   1 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  1   1   1   2   1   1   2   1   1   2   1   1   1   2   2   1   1   1   1   1 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  1   1   2   1   1   1   1   1   2   2   1   1   2   2   2   1   1   1 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 742.4645 695.8635\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 167, 431\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "2  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  1   2   1   2   1   2   1   1   1   1   1   2   2   1   2   2   2   2   2   2 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   1   2   2   2   1 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  1   2   2   1   1   2   2   2   1   1   2   2   2   2   1   2   2   1   2   1 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  1   2   1   2   2   1   1   2   1   2   1   1   1   2   2   2   2   2   1   2 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  2   2   1   2   2   2   2   2   2   2   2   2   1   2   2   2   2   2   1   2 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  2   2   2   2   2   1   2   2   2   2   1   2   1   2   2   2   1   2   2   1 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  1   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   2 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  1   1   1   1   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   1 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  2   2   2   1   2   2   2   2   2   1   2   1   2   2   1   2   1   2   2   2 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   1 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  2   2   2   2   2   2   1   2   2   1   1   2   1   2   1   1   2   2   2   1 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  2   2   1   2   1   2   2   2   2   2   2   2   2   2   1   2   2   2   2   2 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  1   1   1   2   2   2   2   2   1   1   1   1   1   2   1   2   1   1   2   2 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  2   1   1   1   2   2   2   2   2   2   2   2   1   1   1   1   1   2   1   1 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  1   2   2   1   2   2   2   2   2   2   2   2   2   2   2   1   2   1   2   2 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  2   1   2   1   2   2   2   1   1   1   2   2   2   2   2   2   2   2   2   2 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  1   2   2   2   1   2   1   1   1   2   2   1   2   1   1   2   2   2   2   2 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  2   2   2   2   1   2   2   2   2   2   1   2   1   2   1   2   2   2   1   2 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  2   2   2   1   2   1   1   1   2   2   1   1   2   2   2   2   2   2   1   2 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  2   2   2   2   2   2   2   2   2   1   2   2   2   2   2   2   2   2   1   2 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  2   2   1   2   1   2   2   2   1   2   2   2   2   2   2   2   2   2   2   2 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  1   2   2   2   2   2   2   2   2   1   1   2   2   2   2   2   1   2   2   2 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  1   2   2   2   2   2   2   2   1   2   2   1   1   2   1   1   1   2   2   2 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  2   1   1   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   1   2 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  2   2   2   2   2   2   1   2   1   1   2   1   2   2   2   2   2   2   2   2 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  1   2   1   2   2   1   2   2   2   2   2   2   2   2   1   2   2   2   1   1 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  1   1   1   1   1   1   2   1   2   1   1   1   1   1   1   1   2   2   2   2 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  2   2   2   1   2   2   1   2   2   1   2   2   2   1   1   2   2   2   2   2 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  2   2   1   2   2   2   2   2   1   1   2   2   1   1   1   2   2   2 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 695.8635 742.4645\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 431, 167\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "2 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  2   1   2   1   2   1   2   2   2   2   2   1   1   2   1   1   1   1   1   1 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1   1   2 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  2   1   1   2   2   1   1   1   2   2   1   1   1   1   2   1   1   2   1   2 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  2   1   2   1   1   2   2   1   2   1   2   2   2   1   1   1   1   1   2   1 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  1   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   2   1 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  1   1   1   1   1   2   1   1   1   1   2   1   2   1   1   1   2   1   1   2 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  2   2   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  1   1   1   2   1   1   1   1   1   2   1   2   1   1   2   1   2   1   1   1 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  1   1   1   1   1   1   2   1   1   2   2   1   2   1   2   2   1   1   1   2 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  1   1   2   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  2   2   2   1   1   1   1   1   2   2   2   2   2   1   2   1   2   2   1   1 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   2   2 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  2   1   1   2   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  1   2   1   2   1   1   1   2   2   2   1   1   1   1   1   1   1   1   1   1 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  2   1   1   1   2   1   2   2   2   1   1   2   1   2   2   1   1   1   1   1 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  1   1   1   1   2   1   1   1   1   1   2   1   2   1   2   1   1   1   2   1 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  1   1   1   2   1   2   2   2   1   1   2   2   1   1   1   1   1   1   2   1 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   2   1 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  1   1   2   1   2   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  2   1   1   1   1   1   1   1   1   2   2   1   1   1   1   1   2   1   1   1 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  2   1   1   1   1   1   1   1   2   1   1   2   2   1   2   2   2   1   1   1 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  1   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2   1 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  1   1   1   1   1   1   2   1   2   2   1   2   1   1   1   1   1   1   1   1 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  2   1   2   1   1   2   1   1   1   1   1   1   1   1   2   1   1   1   2   2 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  2   2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   1   1   1 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  1   1   1   2   1   1   2   1   1   2   1   1   1   2   2   1   1   1   1   1 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  1   1   2   1   1   1   1   1   2   2   1   1   2   2   2   1   1   1 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 742.4645 695.8635\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 167, 431\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "2  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  1   2   1   2   1   2   1   1   1   1   1   2   2   1   2   2   2   2   2   2 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   1   2   2   2   1 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  1   2   2   1   1   2   2   2   1   1   2   2   2   2   1   2   2   1   2   1 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  1   2   1   2   2   1   1   2   1   2   1   1   1   2   2   2   2   2   1   2 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  2   2   1   2   2   2   2   2   2   2   2   2   1   2   2   2   2   2   1   2 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  2   2   2   2   2   1   2   2   2   2   1   2   1   2   2   2   1   2   2   1 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  1   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   2 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  1   1   1   1   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   1 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  2   2   2   1   2   2   2   2   2   1   2   1   2   2   1   2   1   2   2   2 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1   2   1 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  2   2   2   2   2   2   1   2   2   1   1   2   1   2   1   1   2   2   2   1 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  2   2   1   2   1   2   2   2   2   2   2   2   2   2   1   2   2   2   2   2 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  1   1   1   2   2   2   2   2   1   1   1   1   1   2   1   2   1   1   2   2 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  2   1   1   1   2   2   2   2   2   2   2   2   1   1   1   1   1   2   1   1 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  1   2   2   1   2   2   2   2   2   2   2   2   2   2   2   1   2   1   2   2 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  2   1   2   1   2   2   2   1   1   1   2   2   2   2   2   2   2   2   2   2 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  1   2   2   2   1   2   1   1   1   2   2   1   2   1   1   2   2   2   2   2 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  2   2   2   2   1   2   2   2   2   2   1   2   1   2   1   2   2   2   1   2 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  2   2   2   1   2   1   1   1   2   2   1   1   2   2   2   2   2   2   1   2 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  2   2   2   2   2   2   2   2   2   1   2   2   2   2   2   2   2   2   1   2 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  2   2   1   2   1   2   2   2   1   2   2   2   2   2   2   2   2   2   2   2 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  1   2   2   2   2   2   2   2   2   1   1   2   2   2   2   2   1   2   2   2 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  1   2   2   2   2   2   2   2   1   2   2   1   1   2   1   1   1   2   2   2 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  2   1   1   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   1   2 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  2   2   2   2   2   2   1   2   1   1   2   1   2   2   2   2   2   2   2   2 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  1   2   1   2   2   1   2   2   2   2   2   2   2   2   1   2   2   2   1   1 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  1   1   1   1   1   1   2   1   2   1   1   1   1   1   1   1   2   2   2   2 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  2   2   2   1   2   2   1   2   2   1   2   2   2   1   1   2   2   2   2   2 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  2   2   1   2   2   2   2   2   1   1   2   2   1   1   1   2   2   2 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 695.8635 742.4645\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n",
      "K-means clustering with 2 clusters of sizes 431, 167\n",
      "\n",
      "Cluster means:\n",
      "           R          F          M          T\n",
      "1  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
      "2 -0.0293035  0.8839989  0.8839989  1.1930724\n",
      "\n",
      "Clustering vector:\n",
      "  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n",
      "  2   1   2   1   2   1   2   2   2   2   2   1   1   2   1   1   1   1   1   1 \n",
      " 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1   1   2 \n",
      " 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n",
      "  2   1   1   2   2   1   1   1   2   2   1   1   1   1   2   1   1   2   1   2 \n",
      " 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n",
      "  2   1   2   1   1   2   2   1   2   1   2   2   2   1   1   1   1   1   2   1 \n",
      " 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n",
      "  1   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   2   1 \n",
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n",
      "  1   1   1   1   1   2   1   1   1   1   2   1   2   1   1   1   2   1   1   2 \n",
      "121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n",
      "  2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1 \n",
      "141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n",
      "  2   2   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2 \n",
      "161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n",
      "  1   1   1   2   1   1   1   1   1   2   1   2   1   1   2   1   2   1   1   1 \n",
      "181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   2 \n",
      "201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n",
      "  1   1   1   1   1   1   2   1   1   2   2   1   2   1   2   2   1   1   1   2 \n",
      "221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n",
      "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n",
      "241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n",
      "  1   1   2   1   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1 \n",
      "261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n",
      "  2   2   2   1   1   1   1   1   2   2   2   2   2   1   2   1   2   2   1   1 \n",
      "281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "  1   2   2   2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   2   2 \n",
      "301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n",
      "  2   1   1   2   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1 \n",
      "321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n",
      "  1   2   1   2   1   1   1   2   2   2   1   1   1   1   1   1   1   1   1   1 \n",
      "341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n",
      "  2   1   1   1   2   1   2   2   2   1   1   2   1   2   2   1   1   1   1   1 \n",
      "361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n",
      "  1   1   1   1   2   1   1   1   1   1   2   1   2   1   2   1   1   1   2   1 \n",
      "381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n",
      "  1   1   1   2   1   2   2   2   1   1   2   2   1   1   1   1   1   1   2   1 \n",
      "401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n",
      "  1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   1   2   1 \n",
      "421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n",
      "  1   1   2   1   2   1   1   1   2   1   1   1   1   1   1   1   1   1   1   1 \n",
      "441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n",
      "  2   1   1   1   1   1   1   1   1   2   2   1   1   1   1   1   2   1   1   1 \n",
      "461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n",
      "  2   1   1   1   1   1   1   1   2   1   1   2   2   1   2   2   2   1   1   1 \n",
      "481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n",
      "  1   2   2   1   1   1   1   2   1   1   1   1   1   1   1   1   1   1   2   1 \n",
      "501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n",
      "  1   1   1   1   1   1   2   1   2   2   1   2   1   1   1   1   1   1   1   1 \n",
      "521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n",
      "  2   1   2   1   1   2   1   1   1   1   1   1   1   1   2   1   1   1   2   2 \n",
      "541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n",
      "  2   2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   1   1   1 \n",
      "561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n",
      "  1   1   1   2   1   1   2   1   1   2   1   1   1   2   2   1   1   1   1   1 \n",
      "581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 \n",
      "  1   1   2   1   1   1   1   1   2   2   1   1   2   2   2   1   1   1 \n",
      "\n",
      "Within cluster sum of squares by cluster:\n",
      "[1] 742.4645 695.8635\n",
      " (between_SS / total_SS =  34.5 %)\n",
      "\n",
      "Available components:\n",
      "\n",
      "[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n",
      "[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n"
     ]
    }
   ],
   "source": [
    "#run multiple times\n",
    "#use multiple random assignment to avoid local optimal\n",
    "for (i in 1:10){\n",
    "    km.out=kmeans(trainnorm[,1:4],2, nstart = i * 2)\n",
    "    print(km.out)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.out=kmeans(trainnorm[,1:4],2,nstart = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = km.out$centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>R</th><th scope=col>F</th><th scope=col>M</th><th scope=col>T</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td> 0.2885026</td><td>-0.4699880</td><td>-0.4699880</td><td>-0.3992417</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>-0.0293035</td><td> 0.8839989</td><td> 0.8839989</td><td> 1.1930724</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & R & F & M & T\\\\\n",
       "\\hline\n",
       "\t1 &  0.2885026 & -0.4699880 & -0.4699880 & -0.3992417\\\\\n",
       "\t2 & -0.0293035 &  0.8839989 &  0.8839989 &  1.1930724\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | R | F | M | T | \n",
       "|---|---|\n",
       "| 1 |  0.2885026 | -0.4699880 | -0.4699880 | -0.3992417 | \n",
       "| 2 | -0.0293035 |  0.8839989 |  0.8839989 |  1.1930724 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  R          F          M          T         \n",
       "1  0.2885026 -0.4699880 -0.4699880 -0.3992417\n",
       "2 -0.0293035  0.8839989  0.8839989  1.1930724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnorm$dist1 = apply(trainnorm,1,function(x) sqrt(sum((as.numeric(x[1:4])-centers[1])^2)))\n",
    "trainnorm$dist2 = apply(trainnorm,1,function(x) sqrt(sum((as.numeric(x[1:4])-centers[2])^2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnormd1=trainnorm\n",
    "trainnormd2=trainnorm\n",
    "deter1 = data.frame()\n",
    "deter2=data.frame()\n",
    "for (i in 1:30){\n",
    "    tmpmin =which(trainnormd1$dist1==(min(trainnormd1$dist1)))\n",
    "    if (length(tmpmin)>=1){\n",
    "        tmpmin = tmpmin[1]\n",
    "    }\n",
    "#     print(tmpmin)\n",
    "    deter1=rbind(deter1,trainnormd1[tmpmin,])\n",
    "    trainnormd1=trainnormd1[-tmpmin,]\n",
    "}\n",
    "for (i in 1:30){\n",
    "    tmpmin =which(trainnormd2$dist2==(min(trainnormd2$dist2)))\n",
    "    if (length(tmpmin)>=1){\n",
    "        tmpmin = tmpmin[1]\n",
    "    }\n",
    "#     print(tmpmin)\n",
    "    deter2=rbind(deter2,trainnormd2[tmpmin,])\n",
    "    trainnormd2=trainnormd2[-tmpmin,]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 25\n",
      "[1] 24\n"
     ]
    }
   ],
   "source": [
    "print(length(which(deter1$Class==0)))\n",
    "print(length(which(deter2$Class==0)))\n",
    "#so we have to mark both cluster as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in confusionMatrix.default(ptrain, trainnorm$Class):\n",
      "\"Levels are not in the same order for reference and data. Refactoring data to match.\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 456 142\n",
       "         1   0   0\n",
       "                                          \n",
       "               Accuracy : 0.7625          \n",
       "                 95% CI : (0.7264, 0.7961)\n",
       "    No Information Rate : 0.7625          \n",
       "    P-Value [Acc > NIR] : 0.5225          \n",
       "                                          \n",
       "                  Kappa : 0               \n",
       " Mcnemar's Test P-Value : <2e-16          \n",
       "                                          \n",
       "            Sensitivity : 1.0000          \n",
       "            Specificity : 0.0000          \n",
       "         Pos Pred Value : 0.7625          \n",
       "         Neg Pred Value :    NaN          \n",
       "             Prevalence : 0.7625          \n",
       "         Detection Rate : 0.7625          \n",
       "   Detection Prevalence : 1.0000          \n",
       "      Balanced Accuracy : 0.5000          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ptrain = cl_predict(km.out,trainnorm[,1:4])\n",
    "for (i in 1:length(ptrain)){\n",
    "    ptrain[i]=0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "confusionMatrix(ptrain,trainnorm$Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in confusionMatrix.default(ptest, testnorm$Class):\n",
      "\"Levels are not in the same order for reference and data. Refactoring data to match.\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 114  36\n",
       "         1   0   0\n",
       "                                          \n",
       "               Accuracy : 0.76            \n",
       "                 95% CI : (0.6835, 0.8259)\n",
       "    No Information Rate : 0.76            \n",
       "    P-Value [Acc > NIR] : 0.5446          \n",
       "                                          \n",
       "                  Kappa : 0               \n",
       " Mcnemar's Test P-Value : 5.433e-09       \n",
       "                                          \n",
       "            Sensitivity : 1.00            \n",
       "            Specificity : 0.00            \n",
       "         Pos Pred Value : 0.76            \n",
       "         Neg Pred Value :  NaN            \n",
       "             Prevalence : 0.76            \n",
       "         Detection Rate : 0.76            \n",
       "   Detection Prevalence : 1.00            \n",
       "      Balanced Accuracy : 0.50            \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ptest=cl_predict(km.out, testnorm[,1:4])\n",
    "\n",
    "for (i in 1:length(ptest)){\n",
    "    ptest[i]=0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "confusionMatrix(ptest,testnorm$Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
